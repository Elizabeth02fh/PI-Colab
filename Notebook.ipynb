{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + \"\\\\Dataset\\\\\"\n",
    "files = glob.glob(path + \"/*precios*\")\n",
    "#un solo archivo\n",
    "#new=glob.glob(path + \"/precios_semana_20200413.csv\")#para la carga incremental\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lectura de archivos (FER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios = pd.read_excel(files[0], dtype=object ,sheet_name=1)\n",
    "df_precios1 = pd.read_excel(files[0], sheet_name=0)\n",
    "df_precios2 = pd.read_csv(files[1],sep=',',encoding='utf-16')\n",
    "df_precios3 = pd.read_json(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sucu_file = glob.glob(path + \"/*sucursal*\")\n",
    "sucursal = pd.read_csv(sucu_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de archivos generalizada\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caro: Si no entendi mal, esta funcion no agregaria los datos del csv de sucursal tambien, dado que se fija solo en la extension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path): \n",
    "\n",
    "    for i in path:\n",
    "        ext = os.path.splitext(i)[-1].lower() #deja solo las extensiones\n",
    "        if ext=='.xlsx':\n",
    "            xls = pd.ExcelFile(i)\n",
    "            sheets=xls.sheet_names\n",
    "            if len(sheets)==1:\n",
    "                df=pd.read_excel(i)\n",
    "                return df\n",
    "            elif len(sheets)==2:\n",
    "                df1=pd.read_excel(i,sheet_name=0).assign(semana=sheets[0])\n",
    "                df2=pd.read_excel(i,sheet_name=1).assign(semana=sheets[1])\n",
    "                return df1, df2\n",
    "            else:\n",
    "                print('Archivo Excel con mas de dos sheets')\n",
    "        elif ext=='.csv' or ext=='.txt':\n",
    "            df=pd.read_csv(i,sep=',',encoding='utf-16').assign(semana=os.path.basename(i).split('.')[0])\n",
    "            return df\n",
    "        elif ext=='.json':\n",
    "            df=pd.read_json(i).assign(semana=os.path.basename(i).split('.')[0])\n",
    "            return df\n",
    "        else:\n",
    "            print('no excel ni csv ni json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion\n",
    "\n",
    "-df_precios 1 15156 delete\n",
    "\n",
    "- df_precios 3 despues de etl 2124 nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>FER:</u>  \n",
    "-  La funcion unDate(x) trabajaba a nivel Serie y originalmente estaba pensada para ser usada en un .apply(), por eso no estaba funcionando. Para corregirlo (no estoy seguro de que sea la mejor manera), la renombre a unDateSeries(x) y cree una unDate(df) que recibe un df y aplica unDateSeries a la serie Sucursal_id.  \n",
    "-  La typeCheck() para la parte de producto no funcionaba porque trataba de aplicar la funcion en forma vectorizaa sin tener en cuenta los diferentes casos dentro de una misma columna de manera correcta. Ya modifique y quedó funcionando todo.\n",
    "**A revisar**  \n",
    "La carga de archivos automatizada no contempla el hecho de que la segunda hoja del sheet tiene datos anteriores a la primera hoja (estan invertidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unDate(df):\n",
    "    \n",
    "    def unDateSeries(x):\n",
    "        \"\"\"read_excel me trae algunas sucursal_id como si fueran datetime. Esta funcion toma elementos de una serie, \n",
    "           chequea si son datetime y en caso de serlo, los convierte a str con el formato acorde al orden de identificadores de sucursal\"\"\"\n",
    "        if isinstance(x,datetime):\n",
    "            x=x.strftime(\"%#d-%#m-%Y\")\n",
    "        return x\n",
    "\n",
    "    df.sucursal_id = df.sucursal_id.apply(unDateSeries)\n",
    "    return df \n",
    "\n",
    "def nasDups(df):\n",
    "    \"\"\"Cosas básicas: remover duplicados y nulos\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def typeCheck(df):\n",
    "    \"\"\"En base a la exploracion inicial, se detectaron errores comunes en los diferentes campos. \n",
    "       Esta funcion soluciona todos los problemas encontrados en los diferentes archivos de la fuente de datos\"\"\"\n",
    "\n",
    "    if df.precio.dtype != \"float\":\n",
    "        \"\"\"Algunos precios llegan con un string vacio en vez de nulo. Los convertimos a None para removerlos despues. \n",
    "           A los que tienen valores, los convertimos a float\"\"\"\n",
    "        try:\n",
    "            df.precio = df.precio.apply(lambda x: None if x == '' else x)\n",
    "            df.precio = df.precio.astype(float)\n",
    "        except:\n",
    "            print(\"Error type precios\")\n",
    "\n",
    "    if df.sucursal_id.dtype != \"str\":\n",
    "        try:\n",
    "            df.sucursal_id = df.sucursal_id.astype('string')\n",
    "        except:\n",
    "            print('Error type sucursal_id')\n",
    "            \n",
    "    def prod(x):\n",
    "        if isinstance(x,str):\n",
    "            x = x.split('-')[-1]\n",
    "        elif isinstance(x,float):\n",
    "            x = int(x)\n",
    "        else:    \n",
    "            x=x\n",
    "        return int(x)\n",
    "    df.producto_id = df.producto_id.apply(prod)\n",
    "\n",
    "    return df\n",
    "\n",
    "def splitProd(df):\n",
    "    \"\"\"Algunos productos tenian hardcodeada la sucursal. Removemos esa porcion del string y nos quedamos \n",
    "       unicamente con el codigo de producto (12-13 char)\"\"\"\n",
    "    df.producto_id = df.producto_id.str.split('-').str[-1]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregué etl_sucu que solamente aplica los tipos de datos adecuados para compatiblidad con POSTGRES (no estoy seguro de que sea realmente necesario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(df):\n",
    "    df = nasDups(df)\n",
    "    df = unDate(df)\n",
    "    df = typeCheck(df)\n",
    "    #df = splitProd(df)\n",
    "    return df\n",
    "\n",
    "def etl_sucu(sucursal):    \n",
    "    sucursal.comercioId = sucursal.comercioId.astype(int)\n",
    "    sucursal.banderaId = sucursal.banderaId.astype(int)\n",
    "    sucursal.banderaDescripcion = sucursal.banderaDescripcion.astype('string')\n",
    "    sucursal.comercioRazonSocial = sucursal.comercioRazonSocial.astype('string')\n",
    "    sucursal.provincia = sucursal.provincia.astype('string')\n",
    "    sucursal.localidad = sucursal.localidad.astype('string')\n",
    "    sucursal.direccion = sucursal.direccion.astype('string')\n",
    "    sucursal.sucursalNombre = sucursal.sucursalNombre.astype('string')\n",
    "    sucursal.sucursalTipo = sucursal.sucursalTipo.astype('string')\n",
    "    sucursal.columns = sucursal.columns.str.lower()\n",
    "    return sucursal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de transformación a Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios = etl(df_precios)\n",
    "df_precios1 = etl(df_precios1)\n",
    "df_precios2 = etl(df_precios2)\n",
    "df_precios3 = etl(df_precios3)\n",
    "sucursal = etl_sucu(sucursal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregué el DROP TABLE IF EXISTS nombre_tabla CASCADE para poder ejecutar siempre el comando sin problemas y cambie los nombres de las columnas de sucursal a todo minusculas porque parece que ahi daba error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "tabla_precio = 'DROP TABLE IF EXISTS Precios CASCADE; CREATE TABLE Precios (precio float(10), producto_id int, sucursal_id varchar(10), FOREIGN KEY(sucursal_id) REFERENCES Sucursal(id), FOREIGN KEY(producto_id) REFERENCES Producto(id));'\n",
    "tabla_producto = 'DROP TABLE IF EXISTS Producto CASCADE; CREATE TABLE Producto (id int NOT NULL, marca text, nombre text, presentacion varchar(10), categoria1 text, categoria2 text, categoria3 text, PRIMARY KEY(id));'\n",
    "tabla_sucursal = 'DROP TABLE IF EXISTS Sucursal CASCADE; CREATE TABLE Sucursal (id varchar(12) NOT NULL, comercioid int, banderaid int, banderadescripcion text, comerciorazonsocial text, provincia text, localidad text, direccion text, lat varchar(20), lng varchar(20), sucursalnombre text, sucursaltipo text, PRIMARY KEY(id))'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "   user='admin',\n",
    "   password='admin1234',\n",
    "   database='productDb',\n",
    "   port='5432'\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(tabla_producto)\n",
    "cur.execute(tabla_sucursal)\n",
    "cur.execute(tabla_precio)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carga se ha hecho con exito!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "cone = create_engine('postgresql://admin:admin1234@localhost:5432/productDb', pool_size=50, max_overflow=0)\n",
    "\n",
    "sucursal.to_sql(name='sucursal',con=cone, if_exists='append', index=False)\n",
    "print('La carga se ha hecho con exito!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c4de0fabe0602e2c2e9a8dee706b7aec8ef4c85d03ce0961883d90af556438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
