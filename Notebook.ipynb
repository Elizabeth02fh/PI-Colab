{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + \"\\\\Dataset\\\\\"\n",
    "files = glob.glob(path + \"/*precios*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lectura de archivos (FER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios = pd.read_excel(files[0], dtype=object ,sheet_name=1)\n",
    "df_precios1 = pd.read_excel(files[0], sheet_name=0)\n",
    "df_precios2 = pd.read_csv(files[1],sep=',',encoding='utf-16')\n",
    "df_precios3 = pd.read_json(files[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de archivos generalizada\n",
    "\n",
    "*Aqui me falta agregar los otros tipos de archivos (csv-json-parquet)*\n",
    "La idea es que se lean siempre los archivos del directorio y escoja el metodo de lectura segun la extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no excel\n",
      "no excel\n",
      "no excel\n"
     ]
    }
   ],
   "source": [
    "# def read_file(paths): \n",
    "\"\"\"Esto ya realiza la lectura de un archivo excel con dos sheets, se me atrofio el cerebro pensando en un while si tenia mas de dos jaja\"\"\"\n",
    "\n",
    "for i in files:\n",
    "    ext = os.path.splitext(i)[-1].lower() #deja solo las extensiones\n",
    "    if ext=='.xlsx':\n",
    "        xls = pd.ExcelFile(i)\n",
    "        sheets=xls.sheet_names\n",
    "        if len(sheets)==1:\n",
    "            df=pd.read_excel(i)\n",
    "        elif len(sheets)==2:\n",
    "            df_map= pd.read_excel(i,sheet_name=None) #crea un dic con cada sheet\n",
    "            df1=df_map[sheets[0]]\n",
    "            df2=df_map[sheets[1]]\n",
    "        else:\n",
    "            print('Archivo Excel con mas de dos sheets')\n",
    "    else:\n",
    "        print('no excel')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion\n",
    "\n",
    "Definicion de funciones para realizar transformaciones a los archivos\n",
    "\n",
    "- Aplique primero todo el etl como estaba plantado y vi algunos errores en product_id = ya arreglado dentro de las funciones\n",
    "\n",
    "- La primera funcion unDate aun no arregla el problema (ver en el sheet 2) pero no pude arreglarlo aun  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>FER:</u>  \n",
    "-  La funcion unDate(x) trabajaba a nivel Serie y originalmente estaba pensada para ser usada en un .apply(), por eso no estaba funcionando. Para corregirlo (no estoy seguro de que sea la mejor manera), la renombre a unDateSeries(x) y cree una unDate(df) que recibe un df y aplica unDateSeries a la serie Sucursal_id.  \n",
    "-  La typeCheck() para la parte de producto no funcionaba porque trataba de aplicar la funcion en forma vectorizaa sin tener en cuenta los diferentes casos dentro de una misma columna de manera correcta. Ya modifique y quedó funcionando todo.\n",
    "**A revisar**  \n",
    "La carga de archivos automatizada no contempla el hecho de que la segunda hoja del sheet tiene datos anteriores a la primera hoja (estan invertidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unDate(df):\n",
    "    \n",
    "    def unDateSeries(x):\n",
    "        \"\"\"read_excel me trae algunas sucursal_id como si fueran datetime. Esta funcion toma elementos de una serie, \n",
    "           chequea si son datetime y en caso de serlo, los convierte a str con el formato acorde al orden de identificadores de sucursal\"\"\"\n",
    "        if isinstance(x,datetime):\n",
    "            x=x.strftime(\"%d-%m-%Y\")\n",
    "        return x\n",
    "\n",
    "    df.sucursal_id = df.sucursal_id.apply(unDateSeries)\n",
    "    return df \n",
    "\n",
    "def nasDups(df):\n",
    "    \"\"\"Cosas básicas: remover duplicados y nulos\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def typeCheck(df):\n",
    "    \"\"\"En base a la exploracion inicial, se detectaron errores comunes en los diferentes campos. \n",
    "       Esta funcion soluciona todos los problemas encontrados en los diferentes archivos de la fuente de datos\"\"\"\n",
    "\n",
    "    if df.precio.dtype != \"float\":\n",
    "        \"\"\"Algunos precios llegan con un string vacio en vez de nulo. Los convertimos a None para removerlos despues. \n",
    "           A los que tienen valores, los convertimos a float\"\"\"\n",
    "        try:\n",
    "            df.precio = df.precio.apply(lambda x: None if x == '' else x)\n",
    "            df.precio = df.precio.astype(float)\n",
    "        except:\n",
    "            print(\"Error type precios\")\n",
    "\n",
    "    if df.sucursal_id.dtype != \"str\":\n",
    "        try:\n",
    "            df.sucursal_id = df.sucursal_id.astype('string')\n",
    "        except:\n",
    "            print('Error type sucursal_id')\n",
    "            \n",
    "    def prod(x):\n",
    "        if isinstance(x,str):\n",
    "            x = x.split('-')[-1]\n",
    "        elif isinstance(x,float):\n",
    "            x = int(x)\n",
    "        else:    \n",
    "            x=x\n",
    "        return x\n",
    "    df.producto_id = df.producto_id.apply(prod)\n",
    "\n",
    "    return df\n",
    "\n",
    "def splitProd(df):\n",
    "    \"\"\"Algunos productos tenian hardcodeada la sucursal. Removemos esa porcion del string y nos quedamos \n",
    "       unicamente con el codigo de producto (12-13 char)\"\"\"\n",
    "    df.producto_id = df.producto_id.str.split('-').str[-1]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(df):\n",
    "    df = nasDups(df)\n",
    "    df = unDate(df)\n",
    "    df = typeCheck(df)\n",
    "    #df = splitProd(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicacion de transformacion a Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_precios = etl(df_precios)\n",
    "# df_precios1 = etl(df_precios1)\n",
    "# df_precios2 = etl(df_precios2)\n",
    "# df_precios3 = etl(df_precios3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precio</th>\n",
       "      <th>producto_id</th>\n",
       "      <th>sucursal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.90</td>\n",
       "      <td>2288</td>\n",
       "      <td>2-1-184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.90</td>\n",
       "      <td>2288</td>\n",
       "      <td>2-1-206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>499.99</td>\n",
       "      <td>205870</td>\n",
       "      <td>9-1-430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539.99</td>\n",
       "      <td>205870</td>\n",
       "      <td>9-2-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539.99</td>\n",
       "      <td>205870</td>\n",
       "      <td>09-03-5218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458538</th>\n",
       "      <td>139.99</td>\n",
       "      <td>9569753142128</td>\n",
       "      <td>25-01-2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458539</th>\n",
       "      <td>34.99</td>\n",
       "      <td>9795403001143</td>\n",
       "      <td>25-01-2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458540</th>\n",
       "      <td>312.50</td>\n",
       "      <td>9990385651922</td>\n",
       "      <td>05-01-2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458541</th>\n",
       "      <td>312.50</td>\n",
       "      <td>9990385651939</td>\n",
       "      <td>05-01-2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458542</th>\n",
       "      <td>198.90</td>\n",
       "      <td>9990385651946</td>\n",
       "      <td>05-01-2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456736 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        precio    producto_id sucursal_id\n",
       "0        29.90           2288     2-1-184\n",
       "1        39.90           2288     2-1-206\n",
       "2       499.99         205870     9-1-430\n",
       "3       539.99         205870     9-2-107\n",
       "4       539.99         205870  09-03-5218\n",
       "...        ...            ...         ...\n",
       "458538  139.99  9569753142128  25-01-2001\n",
       "458539   34.99  9795403001143  25-01-2001\n",
       "458540  312.50  9990385651922  05-01-2003\n",
       "458541  312.50  9990385651939  05-01-2003\n",
       "458542  198.90  9990385651946  05-01-2003\n",
       "\n",
       "[456736 rows x 3 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_precios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cone = create_engine('postgresql://admin:admin1234@localhost:5432/productDb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios.to_sql(name='producto',con=cone, if_exists='append')\n",
    "print('La carga se ha hecho con exito!')\n",
    "\n",
    "#si dice carga con exito pero no lo encontre en mi pgadmin "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c4de0fabe0602e2c2e9a8dee706b7aec8ef4c85d03ce0961883d90af556438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
